{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4bd4099-5272-42c6-a34f-05ba4e409300",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4bd4099-5272-42c6-a34f-05ba4e409300",
        "outputId": "fc8ef093-e273-4c5c-8c50-f2ac6ce4066a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eEPCYIqoR3eX",
      "metadata": {
        "id": "eEPCYIqoR3eX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "source_folder = '/content/drive/MyDrive/Thesis_GitHub_Code'\n",
        "destination_folder = '/content/Thesis_GitHub_Code'\n",
        "\n",
        "if not os.path.exists(destination_folder):\n",
        "    os.makedirs(destination_folder)\n",
        "\n",
        "for item in os.listdir(source_folder):\n",
        "    source_path = os.path.join(source_folder, item)\n",
        "    destination_path = os.path.join(destination_folder, item)\n",
        "    if os.path.isdir(source_path):\n",
        "        shutil.copytree(source_path, destination_path)\n",
        "    else:\n",
        "        shutil.copy2(source_path, destination_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "r0klxHeXbZn_",
      "metadata": {
        "id": "r0klxHeXbZn_"
      },
      "outputs": [],
      "source": [
        "!pip install -r /content/Thesis_GitHub_Code/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fHESTzxbiZnQ",
      "metadata": {
        "id": "fHESTzxbiZnQ"
      },
      "outputs": [],
      "source": [
        "!pip install -q datasets git+https://github.com/huggingface/transformers.git@main\n",
        "!pip install -q encodec\n",
        "!pip install -q soundfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xnd6sgeGVtEu",
      "metadata": {
        "id": "xnd6sgeGVtEu"
      },
      "outputs": [],
      "source": [
        "# importing modules for GPT2 and audio tokenization\n",
        "!cp Thesis_GitHub_Code/model.py /content\n",
        "!cp Thesis_GitHub_Code/utils.py /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pmoJRfeIXcUu",
      "metadata": {
        "id": "pmoJRfeIXcUu"
      },
      "outputs": [],
      "source": [
        "from dataset_splits import split_dataset_files, clas_dict, print_random_file_details\n",
        "from model import AudioGPT2\n",
        "from utils import augment, encode_audio, apply_delay_pattern, remove_delay_pattern"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4322f5c-9921-40d7-b8bd-913df1f5250e",
      "metadata": {
        "id": "b4322f5c-9921-40d7-b8bd-913df1f5250e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import librosa\n",
        "import numpy as np\n",
        "from datasets import load_dataset, Audio\n",
        "from transformers import EncodecModel, AutoProcessor, GPT2Config, GPT2Model\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "# used to define neural network layers and setting up and training the network\n",
        "from IPython.display import Audio as IPyAudio\n",
        "import soundfile as sf\n",
        "import csv\n",
        "from typing import List\n",
        "from tqdm import tqdm # for progress bars\n",
        "import soundfile as sf\n",
        "from audiomentations import Compose, AddGaussianNoise\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0b089d1-da55-451a-918b-01b45337c4f2",
      "metadata": {
        "id": "c0b089d1-da55-451a-918b-01b45337c4f2"
      },
      "outputs": [],
      "source": [
        "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"FoleyGen_Oct.ipynb\"\n",
        "wandb.login(key = '') # Insert your own wandb login key from your accout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81a0d0f3-60b5-464e-b0b1-576851fcee70",
      "metadata": {
        "id": "81a0d0f3-60b5-464e-b0b1-576851fcee70"
      },
      "outputs": [],
      "source": [
        "dataset_path = \"Thesis_GitHub_Code/DataSet\"\n",
        "output_csv_path = \"Thesis_GitHub_Code/dataset_splits.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_cB2mTgRivAZ",
      "metadata": {
        "id": "_cB2mTgRivAZ"
      },
      "outputs": [],
      "source": [
        "# Creating a dictionary for all Foley categories\n",
        "clas_dict = {\n",
        "    \"DogBark\": 0,\n",
        "    \"Footstep\": 1,\n",
        "    \"Gunshot\": 2,\n",
        "    \"Keyboard\": 3,\n",
        "    \"MovingMotorVehicle\": 4,\n",
        "    \"Rain\": 5,\n",
        "    \"SneezeCough\": 6,\n",
        "}\n",
        "\n",
        "# Splitting datasets into train, test and validation and printing them to a .csv\n",
        "def split_dataset_files(dataset_path: str,output_csv_path: str, train_ratio: float = 0.8, test_ratio: float = 0.1, seed: int = None):\n",
        "    training_files: List[dict] = []\n",
        "    valid_files: List[dict] = []\n",
        "    test_files: List[dict] = []\n",
        "\n",
        "    all_files = []\n",
        "    for root, dirs, files in os.walk(dataset_path):\n",
        "        for file in files:\n",
        "            if file.endswith(\".wav\"):\n",
        "                class_id = clas_dict.get(os.path.basename(root), None)\n",
        "                if class_id is not None:\n",
        "                    all_files.append({\"class_id\": class_id, \"file_path\": os.path.join(root, file)})\n",
        "\n",
        "    if seed is not None:\n",
        "        random.seed(seed)\n",
        "    random.shuffle(all_files)\n",
        "\n",
        "    total_files = len(all_files)\n",
        "    num_train = int(total_files * train_ratio)\n",
        "    num_test = int(total_files * test_ratio)\n",
        "    num_valid = total_files - num_train - num_test\n",
        "\n",
        "    training_files = all_files[:num_train]\n",
        "    test_files = all_files[num_train:num_train + num_test]\n",
        "    valid_files = all_files[num_train + num_test:]\n",
        "\n",
        "    with open(output_csv_path, \"w\", newline=\"\") as csv_file:\n",
        "        writer = csv.writer(csv_file)\n",
        "        writer.writerow([\"filepath\", \"split\"])\n",
        "        for file_info in training_files:\n",
        "            writer.writerow([file_info[\"file_path\"], \"train\"])\n",
        "        for file_info in test_files:\n",
        "            writer.writerow([file_info[\"file_path\"], \"test\"])\n",
        "        for file_info in valid_files:\n",
        "            writer.writerow([file_info[\"file_path\"], \"validation\"])\n",
        "\n",
        "    return training_files, valid_files, test_files\n",
        "\n",
        "# For ensuring proper splitting\n",
        "def print_random_file_details(training_files, valid_files, test_files):\n",
        "    \"\"\"\n",
        "    Prints details of one random audio file from either the training, validation, or test set.\n",
        "    \"\"\"\n",
        "    all_files = [\n",
        "        {\"file\": file, \"split\": \"train\"} for file in training_files\n",
        "    ] + [\n",
        "        {\"file\": file, \"split\": \"validation\"} for file in valid_files\n",
        "    ] + [\n",
        "        {\"file\": file, \"split\": \"test\"} for file in test_files\n",
        "    ]\n",
        "\n",
        "    random_file = random.choice(all_files)\n",
        "\n",
        "    print(f\"Randomly Selected File:\")\n",
        "    print(f\"File Path: {random_file['file']['file_path']}\")\n",
        "    print(f\"Class ID: {random_file['file']['class_id']}\")\n",
        "    print(f\"Split: {random_file['split']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fug3Z858DTWS",
      "metadata": {
        "id": "fug3Z858DTWS"
      },
      "outputs": [],
      "source": [
        "# Split the dataset\n",
        "training_files, valid_files, test_files = training_files, valid_files, test_files = split_dataset_files(dataset_path, output_csv_path, train_ratio=0.8, test_ratio=0.1, seed=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "myz-sLxMTuMg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myz-sLxMTuMg",
        "outputId": "f2ed2ec0-3dc6-4234-ab3b-f76159806101"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Randomly Selected File:\n",
            "File Path: Thesis_GitHub_Code/DataSet/SneezeCough/681.wav\n",
            "Class ID: 6\n",
            "Split: train\n"
          ]
        }
      ],
      "source": [
        "print_random_file_details(training_files, valid_files, test_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efbd3f68-4ca4-46c2-90db-0653caa29079",
      "metadata": {
        "id": "efbd3f68-4ca4-46c2-90db-0653caa29079"
      },
      "source": [
        "## Initalizing Encodec Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1a7f1f7-9002-4d33-aa1a-1d49567168c1",
      "metadata": {
        "id": "e1a7f1f7-9002-4d33-aa1a-1d49567168c1"
      },
      "outputs": [],
      "source": [
        "# Initialize EnCodec model\n",
        "from encodec import EncodecModel\n",
        "\n",
        "encodec_model = EncodecModel.encodec_model_24khz()\n",
        "encodec_model.set_target_bandwidth(6.0)\n",
        "\n",
        "# Define codebook_size and num_quantizers based on the actual model\n",
        "codebook_size = 1024\n",
        "num_quantizers = 8"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31fdc09f-286b-4445-b240-13610f25acad",
      "metadata": {
        "id": "31fdc09f-286b-4445-b240-13610f25acad"
      },
      "source": [
        "## Create AudioDataset Class and Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83bb5136-68e2-4c6a-85fb-43f1af636cb2",
      "metadata": {
        "id": "83bb5136-68e2-4c6a-85fb-43f1af636cb2"
      },
      "outputs": [],
      "source": [
        "class AudioDataset(Dataset):\n",
        "    def __init__(self, file_list, encodec_model, max_length=300, codebook_size=1024, apply_augmentation = False):\n",
        "        self.file_list = file_list\n",
        "        self.encodec_model = encodec_model\n",
        "        self.max_length = max_length\n",
        "        self.codebook_size = codebook_size\n",
        "        self.vocab_size = self.codebook_size + 1  # +1 for padding\n",
        "        self.num_quantizers = 8\n",
        "        self.apply_augmentation = apply_augmentation\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_info = self.file_list[idx]\n",
        "        file_path = file_info[\"file_path\"]\n",
        "        class_id = file_info[\"class_id\"]  # Retrieve the class ID\n",
        "        codes = encode_audio(file_path, self.encodec_model, apply_augmentation=self.apply_augmentation)\n",
        "        delayed_codes = apply_delay_pattern(codes,self.codebook_size, self.num_quantizers)\n",
        "\n",
        "        # Truncate or pad sequences to max_length\n",
        "        input_ids = delayed_codes\n",
        "        padding_value = self.vocab_size - 1\n",
        "        if input_ids.shape[0] > self.max_length:\n",
        "            input_ids = input_ids[:self.max_length, :]\n",
        "        else:\n",
        "            pad_length = self.max_length - input_ids.shape[0]\n",
        "            padding = torch.full((pad_length, input_ids.shape[1]), padding_value, dtype=torch.long)\n",
        "            input_ids = torch.cat([input_ids, padding], dim=0)\n",
        "\n",
        "        return input_ids, class_id  # Return the class ID along with the input_ids\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create Data Loaders\n",
        "max_sequence_length = 300\n",
        "batch_size = 4\n",
        "\n",
        "train_dataset = AudioDataset(training_files, encodec_model, max_length=max_sequence_length, apply_augmentation = True) # Apply augmentation only to training files\n",
        "valid_dataset = AudioDataset(valid_files, encodec_model, max_length=max_sequence_length, apply_augmentation = False)\n",
        "test_dataset = AudioDataset(test_files, encodec_model, max_length=max_sequence_length, apply_augmentation = False)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29176b89-4cad-4462-8cf0-1bae5bb6e43a",
      "metadata": {
        "id": "29176b89-4cad-4462-8cf0-1bae5bb6e43a"
      },
      "source": [
        "## Define AudioGPT2 Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26a8fc5d-9c47-4af6-9d4d-98a41c8a084a",
      "metadata": {
        "id": "26a8fc5d-9c47-4af6-9d4d-98a41c8a084a"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc588d7e-fab5-45a0-a500-68a447dc3deb",
      "metadata": {
        "id": "dc588d7e-fab5-45a0-a500-68a447dc3deb"
      },
      "outputs": [],
      "source": [
        "# Initialize the model, loss function, and optimizer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = AudioGPT2(num_quantizers=num_quantizers, codebook_size=codebook_size).to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=model.vocab_size - 1)  # Ignore padding token\n",
        "optimizer = optim.Adam(model.parameters(), lr=5e-5, weight_decay = 1e-4) # applying learning rate, l2 regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f1518c5-73a4-4eab-b42f-64c89eccb06c",
      "metadata": {
        "id": "1f1518c5-73a4-4eab-b42f-64c89eccb06c"
      },
      "source": [
        "#### For starting a new training run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c89b48f5-9536-47cc-b9c6-a94ec43d9abd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "collapsed": true,
        "id": "c89b48f5-9536-47cc-b9c6-a94ec43d9abd",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "dd951f3d-a160-42ce-c9ab-56c11905c263",
        "tags": []
      },
      "outputs": [],
      "source": [
        "num_epochs = 300\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
        "        input_ids, class_id = batch\n",
        "        input_ids, class_id = input_ids.to(device), class_id.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "\n",
        "\n",
        "        # Prepare inputs and targets\n",
        "        inputs = input_ids[:, :-1, :]  # [batch_size, seq_length-1, num_quantizers]\n",
        "        targets = input_ids[:, 1:, :]  # [batch_size, seq_length-1, num_quantizers]\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(input_ids = inputs, class_id = class_id)  # [batch_size, seq_length-1, total_vocab_size]\n",
        "\n",
        "        # Reshape logits and targets\n",
        "        batch_size, seq_length_minus1, _ = inputs.shape\n",
        "        logits = logits.reshape(batch_size * seq_length_minus1, model.total_vocab_size)\n",
        "        targets = targets.reshape(batch_size * seq_length_minus1, model.num_quantizers)\n",
        "\n",
        "        # Compute loss per quantizer\n",
        "        loss = 0\n",
        "        for q in range(model.num_quantizers):\n",
        "            q_targets = targets[:, q]  # [batch_size * seq_length_minus1]\n",
        "            q_offset = q * model.vocab_size\n",
        "            q_logits = logits[:, q_offset : q_offset + model.vocab_size]  # [batch_size * seq_length_minus1, vocab_size]\n",
        "            loss += criterion(q_logits, q_targets)\n",
        "        loss = loss / model.num_quantizers  # Average over quantizers\n",
        "\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {avg_loss:.4f}\")\n",
        "    wandb.log({\"Training Loss\": avg_loss, \"Epoch\": epoch + 1})\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(valid_loader, desc=f\"Validation Epoch {epoch+1}\"):\n",
        "            input_ids, class_id = batch\n",
        "            input_ids, class_id = input_ids.to(device), class_id.to(device)\n",
        "            inputs = input_ids[:, :-1, :]\n",
        "            targets = input_ids[:, 1:, :]\n",
        "            logits = model(input_ids=inputs, class_id=class_id)\n",
        "            batch_size, seq_length_minus1, _ = inputs.shape\n",
        "            logits = logits.reshape(batch_size * seq_length_minus1, model.total_vocab_size)\n",
        "            targets = targets.reshape(batch_size * seq_length_minus1, model.num_quantizers)\n",
        "\n",
        "            loss = 0\n",
        "            for q in range(model.num_quantizers):\n",
        "                q_targets = targets[:, q]\n",
        "                q_offset = q * model.vocab_size\n",
        "                q_logits = logits[:, q_offset : q_offset + model.vocab_size]\n",
        "                loss += criterion(q_logits, q_targets)\n",
        "            loss = loss / model.num_quantizers\n",
        "            val_loss += loss.item()\n",
        "    avg_val_loss = val_loss / len(valid_loader)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Loss: {avg_val_loss:.4f}\")\n",
        "    wandb.log({\"Validation Loss\": avg_val_loss, \"Epoch\": epoch + 1})\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        checkpoint_path = f\"augmodel_checkpoint_l2_{epoch + 1}.pth\"\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': avg_loss,\n",
        "        }, checkpoint_path)\n",
        "        print(f\"Checkpoint saved for epoch {epoch + 1} at {checkpoint_path}\")\n",
        "\n",
        "wandb.finish()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dff0c636-2d36-408c-8434-996eb02aab20",
      "metadata": {
        "id": "dff0c636-2d36-408c-8434-996eb02aab20"
      },
      "source": [
        "### Generating audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "688c04db-73b8-4fd0-84f2-8e7ca430a00e",
      "metadata": {
        "id": "688c04db-73b8-4fd0-84f2-8e7ca430a00e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import soundfile as sf\n",
        "from IPython.display import Audio as IPyAudio\n",
        "\n",
        "checkpoint_path = \"/scratch/ssr9055/my_env/BEST_CHECKPOINT_AUG_L2.pth\" # Replace with the last saved checkpoint\n",
        "\n",
        "# Function to remove delay pattern\n",
        "def remove_delay_pattern(delayed_codes, num_quantizers):\n",
        "    num_frames = delayed_codes.shape[0] - (num_quantizers - 1)\n",
        "    codes = torch.zeros(num_frames, num_quantizers, dtype=delayed_codes.dtype)\n",
        "    for q in range(num_quantizers):\n",
        "        codes[:, q] = delayed_codes[q:q + num_frames, q]\n",
        "    return codes  # Shape: [num_frames, num_quantizers]\n",
        "\n",
        "# Function to generate audio\n",
        "def generate_audio(model, encodec_model, class_id, num_quantizers=8, codebook_size=1024, max_length=600, temperature=1.0, device='cpu'):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    encodec_model.eval()  # Set encodec_model to evaluation mode\n",
        "\n",
        "    model.to(device)  # Move model to the specified device\n",
        "\n",
        "    # Start with padding tokens for each quantizer\n",
        "    start_token = codebook_size  # Padding token index\n",
        "    input_ids = torch.full((1, 1, num_quantizers), start_token, dtype=torch.long, device=device)  # Shape: [1, 1, num_quantizers]\n",
        "\n",
        "    # Convert the class_id to a tensor and move to device\n",
        "    class_id_tensor = torch.tensor([class_id], device=device)\n",
        "\n",
        "    generated = []  # List to hold generated tokens\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculations for generation\n",
        "        for _ in tqdm(range(max_length), desc=\"Generating Audio\"):\n",
        "            # Forward pass through the model, passing class_id_tensor\n",
        "            logits = model(input_ids=input_ids, class_id=class_id_tensor)  # [1, seq_length, total_vocab_size]\n",
        "            logits = logits[:, -1, :]  # [1, total_vocab_size] - Get the logits for the last time step\n",
        "\n",
        "            # Apply temperature to control randomness\n",
        "            logits = logits / temperature\n",
        "\n",
        "            next_tokens = []  # List to store the next token for each quantizer\n",
        "\n",
        "            # Sample next token for each quantizer\n",
        "            for q in range(num_quantizers):\n",
        "                q_offset = q * (codebook_size + 1)\n",
        "                q_logits = logits[:, q_offset:q_offset + codebook_size + 1]  # [1, vocab_size]\n",
        "                q_probs = F.softmax(q_logits, dim=-1)  # Convert logits to probabilities\n",
        "                q_next_token = torch.multinomial(q_probs, num_samples=1)  # Sample next token\n",
        "                q_next_token = q_next_token.squeeze(1)  # Remove extra dimension\n",
        "\n",
        "                # If the sampled token is the padding token, replace it with a valid token\n",
        "                q_next_token_value = q_next_token.item()\n",
        "                if q_next_token_value == codebook_size:\n",
        "                    q_next_token_value = torch.randint(0, codebook_size, (1,)).item()\n",
        "\n",
        "                next_tokens.append(torch.tensor([q_next_token_value], device=device, dtype=torch.long))\n",
        "\n",
        "            # Stack the next tokens for each quantizer and append to generated sequence\n",
        "            next_tokens = torch.stack(next_tokens, dim=1)  # Shape: [1, num_quantizers]\n",
        "            generated.append(next_tokens.squeeze(0))  # Append generated tokens\n",
        "            input_ids = torch.cat([input_ids, next_tokens.unsqueeze(0)], dim=1)  # Update input_ids for the next time step\n",
        "\n",
        "    # Stack the generated tokens to form the final token sequence\n",
        "    generated_tokens = torch.stack(generated, dim=0)  # [seq_length, num_quantizers]\n",
        "\n",
        "    # Remove the delay pattern from generated tokens\n",
        "    codes = remove_delay_pattern(generated_tokens, num_quantizers)\n",
        "\n",
        "    # Add batch dimension to match the input shape for decode\n",
        "    codes = codes.unsqueeze(0)\n",
        "\n",
        "    # Ensure codes are within valid codebook size\n",
        "    codes = codes.clamp(0, codebook_size - 1)\n",
        "\n",
        "    # Decode the audio using the EnCodec model\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            codes = codes.permute(0, 2, 1)  # [batch_size, num_quantizers, num_frames]\n",
        "            encoded_frames = [(codes.to(next(encodec_model.parameters()).device), None)]\n",
        "            decoded_audio = encodec_model.decode(encoded_frames)\n",
        "            audio = decoded_audio.squeeze().cpu().detach().numpy()  # Convert to numpy array\n",
        "        return audio\n",
        "    except Exception as e:\n",
        "        print(f\"Error during decoding: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "max_length = 350  # Adjust as needed\n",
        "temperature = 0.7  # Adjust as needed\n",
        "\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# Loop over each category in clas_dict\n",
        "for class_name, class_id in clas_dict.items():\n",
        "    print(f\"Generating audio for category: {class_name}\")\n",
        "\n",
        "    generated_audio = generate_audio(\n",
        "        model,\n",
        "        encodec_model,\n",
        "        class_id=class_id,  # Pass the class_id for each category\n",
        "        num_quantizers=num_quantizers,\n",
        "        codebook_size=codebook_size,\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    if generated_audio is not None:\n",
        "        # Save the generated audio to a file (optional)\n",
        "        output_filename = f'generated_audio_{class_name}.wav'\n",
        "        sf.write(output_filename, generated_audio, 24000)\n",
        "\n",
        "        # Display the audio\n",
        "        display(IPyAudio(output_filename))\n",
        "    else:\n",
        "        print(f\"Audio generation failed for category: {class_name}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
